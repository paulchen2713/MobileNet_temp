# -*- coding: utf-8 -*-
"""
Created on Tue Jun 7 20:36:42 2021

@author: me
@file: Emotion_Detection.ipy

@dependencies: 
    conda env: tf2
    tf.__version__: 2.1.0
    tf.keras.__version__: 2.2.4
    keras.__version__: 2.3.1
"""
# MobileNet_Emotion_Detection

import numpy as np
import tensorflow as tf
from tensorflow import keras
# from tensorflow.keras.models import Sequential

from tensorflow.keras.layers import Activation, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import categorical_crossentropy
from tensorflow.keras.preprocessing.image import ImageDataGenerator

from tensorflow.keras.preprocessing import image 
# tf.keras.preprocessing.image.load_img(image_path)
from tensorflow.keras.models import Model
from tensorflow.keras.applications import imagenet_utils
# tf.keras.applications.imagenet_utils.decode_predictions(preds, top=5)

from IPython.display import Image
from sklearn.metrics import confusion_matrix

import itertools
import os
import shutil
import random
# import glob
import matplotlib.pyplot as plt
%matplotlib inline

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

"""
(noticed: the following codes activate only when using GPU to run the model, 
 but I'm not using GPU support I think? )
"""
# check to be sure that TensorFlow is able to identify the GPU

physical_devices = tf.config.experimental.list_physical_devices('GPU')
print("Num GPUs Available: ", len(physical_devices))
# Num GPUs Available:  0

# set_memory_growth() allocate only as much GPU memory as needed at a given time, 
# and continues to allocate more when needed

# tf.config.experimental.set_memory_growth(physical_devices[0], True)

# (IndexError                                Traceback (most recent call last)
# <ipython-input-3-df3199446273> in <module>
#      37 # set_memory_growth() allocate only as much GPU memory as needed at a given time,
#      38 # and continues to allocate more when needed
# ---> 39 tf.config.experimental.set_memory_growth(physical_devices[0], True)

# IndexError: list index out of range)

# downloading a copy of a single pretrained MobileNet, 
# with weights that were saved from being trained on ImageNet images
mobile = tf.keras.applications.mobilenet.MobileNet()

# prepare_image() takes an file name, and processes the image to get it in a format that MobileNet expects
def prepare_image(file):
    img_path = 'D:/Deep_Learning_Projects/MobileNet-test-samples/'  # defining the relative path to the images
    # load_img(img_path) takes the image file, and resizing it to be of size (224, 224), 
    # and returns an instance of a PIL image
    img = tf.keras.preprocessing.image.load_img(img_path + file, target_size=(224, 224))
    img_array = keras.preprocessing.image.img_to_array(img)    # then converting the PIL image into an array
    # input_array = np.array([input_array])                    # converting single image to a batch

    # numpy.expand_dims(a, axis) insert a new dimension in "axis th" dimension, and 
    # all previous dimensions would be push to the right
    img_array_dims_expand = np.expand_dims(img_array, axis=0)
    # preprocess_inout() scale the pixel values in the image between -1 to 1, same format as the images that 
    # MobileNet was originally trained on, and return the preprocessed image data as a numpy array
    return tf.keras.applications.mobilenet.preprocess_input(img_array_dims_expand)

# just showing the test image on screen, test1 here is a wireless keyboard
# from IPython.display import Image
Image(filename='D:/Deep_Learning_Projects/MobileNet-test-samples/test1.jpg', width=300,height=200)

# geting some predictions from MobileNet and see how it works
preprocessed_image = prepare_image('test1.jpg') 
predictions = mobile.predict(preprocessed_image) # Model.predict() returns values of each class's probability
# decode_predictions() returns a list of lists of tuples [[(class_name, class_description, score), ...], ...]
# for top class prediction, in other words, it returns the top 5 ImageNet class predictions with the ImageNet 
# class ID, the class label, and the probability, respectively
results = tf.keras.applications.imagenet_utils.decode_predictions(predictions, top=5)
# print(results)

# organize data into train, valid, and test dirs, automatically
os.chdir('D:/Deep_Learning_Projects/Emotion_Detection/Emotion-Detection-Dataset')
if os.path.isdir('train/0/') is False:
    os.mkdir('train')
    os.mkdir('valid')
    os.mkdir('test')
    
    for i in range(0, 7):
        shutil.move(f'{i}', 'train')
        os.mkdir(f'valid/{i}')
        os.mkdir(f'test/{i}')

        valid_samples = random.sample(os.listdir(f'train/{i}'), 30)
        for j in valid_samples:
            shutil.move(f'train/{i}/{j}', f'valid/{i}')

        test_samples = random.sample(os.listdir(f'train/{i}'), 5)
        for j in test_samples:
            shutil.move(f'train/{i}/{j}', f'test/{i}')

os.chdir('../..')

# assign the path to path variables for upcoming preprocessing
train_path = 'D:/Deep_Learning_Projects/Emotion_Detection/Emotion-Detection-Dataset/train'
valid_path = 'D:/Deep_Learning_Projects/Emotion_Detection/Emotion-Detection-Dataset/valid'
test_path  = 'D:/Deep_Learning_Projects/Emotion_Detection/Emotion-Detection-Dataset/test'

# use Keras' ImageDataGenerator class to create batches of data from the train, valid, and test directories.
# mind the target_size = (224, 224), or (48, 48)
train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input) \
    .flow_from_directory(directory=train_path, target_size=(48, 48), batch_size=10)
valid_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input) \
    .flow_from_directory(directory=valid_path, target_size=(48, 48), batch_size=10)
test_batches  = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input) \
    .flow_from_directory(directory=test_path,  target_size=(48, 48), batch_size=10, shuffle=False)

print(train_batches.classes.shape)
print(valid_batches.classes.shape)
print(test_batches.classes.shape)
# (2441,)
# (210,)
# (35,)

imgs, labels = next(train_batches)
def plotImages(images_arr):
    fig, axes = plt.subplots(1, 10, figsize=(20,20))
    axes = axes.flatten()
    for img, ax in zip(images_arr, axes):
        ax.imshow(img)
        ax.axis('off')
    plt.tight_layout()
    plt.show()

plotImages(imgs)
print(labels)

print(mobile.summary())

x = mobile.layers[-6].output

# functional model
output_layer = Dense(uints=7, activation='softmax')(x)

model = Model(inputs=mobile.input, outputs=output_layer)

# tunable hyper-parameter --> 25
for layer in model.layers[:-25]:
    layer.trainable = False

print(mobile.summary())

model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])

model.fit(x=train_batches, 
        step_per_epoch=len(train_batches), 
        validation_data=valid_batches,
        validation_steps=len(valid_batches),
        epoches=30,
        verbose=2
)
# severe overfit...
# Train for 245 steps, validate for 21 steps
# Epoch 1/30
# 245/245 - 374s - loss: 1.5718 - accuracy: 0.3859 - val_loss: 2.8528 - val_accuracy: 0.3000
# Epoch 2/30
# 245/245 - 366s - loss: 0.5321 - accuracy: 0.8849 - val_loss: 2.9035 - val_accuracy: 0.3095
# Epoch 3/30
# 245/245 - 369s - loss: 0.2010 - accuracy: 0.9947 - val_loss: 3.0462 - val_accuracy: 0.3095
# Epoch 4/30
# 245/245 - 369s - loss: 0.0888 - accuracy: 1.0000 - val_loss: 3.1220 - val_accuracy: 0.3048
# Epoch 5/30
# 245/245 - 368s - loss: 0.0489 - accuracy: 1.0000 - val_loss: 3.0192 - val_accuracy: 0.3333
# Epoch 6/30
# 245/245 - 367s - loss: 0.0315 - accuracy: 1.0000 - val_loss: 3.0804 - val_accuracy: 0.3667
# Epoch 7/30
# 245/245 - 368s - loss: 0.0218 - accuracy: 1.0000 - val_loss: 3.1756 - val_accuracy: 0.3619
# Epoch 8/30
# 245/245 - 368s - loss: 0.0159 - accuracy: 1.0000 - val_loss: 3.2090 - val_accuracy: 0.3524
# Epoch 9/30
# 245/245 - 368s - loss: 0.0120 - accuracy: 1.0000 - val_loss: 3.2395 - val_accuracy: 0.3619
# Epoch 10/30
# 245/245 - 367s - loss: 0.0093 - accuracy: 1.0000 - val_loss: 3.2855 - val_accuracy: 0.3476
# Epoch 11/30
# 245/245 - 368s - loss: 0.0073 - accuracy: 1.0000 - val_loss: 3.3098 - val_accuracy: 0.3524
# Epoch 12/30
# 245/245 - 368s - loss: 0.0058 - accuracy: 1.0000 - val_loss: 3.3762 - val_accuracy: 0.3571
# Epoch 13/30
# 245/245 - 375s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 3.4122 - val_accuracy: 0.3667
# Epoch 14/30
# 245/245 - 373s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 3.4324 - val_accuracy: 0.3619
# Epoch 15/30
# 245/245 - 404s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 3.4553 - val_accuracy: 0.3571
# Epoch 16/30
# 245/245 - 376s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 3.4792 - val_accuracy: 0.3619
# Epoch 17/30
# 245/245 - 368s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.5444 - val_accuracy: 0.3714
# Epoch 18/30
# 245/245 - 367s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.5626 - val_accuracy: 0.3667
# Epoch 19/30
# 245/245 - 366s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 3.5736 - val_accuracy: 0.3762
# Epoch 20/30
# 245/245 - 368s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 3.6427 - val_accuracy: 0.3714
# Epoch 21/30
# 245/245 - 368s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.6891 - val_accuracy: 0.3762
# Epoch 22/30
# 245/245 - 369s - loss: 9.3875e-04 - accuracy: 1.0000 - val_loss: 3.7031 - val_accuracy: 0.3619
# Epoch 23/30
# 245/245 - 368s - loss: 8.0026e-04 - accuracy: 1.0000 - val_loss: 3.7493 - val_accuracy: 0.3714
# Epoch 24/30
# 245/245 - 368s - loss: 6.8257e-04 - accuracy: 1.0000 - val_loss: 3.7586 - val_accuracy: 0.3667
# Epoch 25/30
# 245/245 - 367s - loss: 5.8373e-04 - accuracy: 1.0000 - val_loss: 3.7741 - val_accuracy: 0.3667
# Epoch 26/30
# 245/245 - 367s - loss: 4.9997e-04 - accuracy: 1.0000 - val_loss: 3.8296 - val_accuracy: 0.3714
# Epoch 27/30
# 245/245 - 367s - loss: 4.2978e-04 - accuracy: 1.0000 - val_loss: 3.8471 - val_accuracy: 0.3762
# Epoch 28/30
# 245/245 - 368s - loss: 3.6909e-04 - accuracy: 1.0000 - val_loss: 3.8947 - val_accuracy: 0.3714
# Epoch 29/30
# 245/245 - 368s - loss: 3.1750e-04 - accuracy: 1.0000 - val_loss: 3.9154 - val_accuracy: 0.3762
# Epoch 30/30
# 245/245 - 368s - loss: 2.7406e-04 - accuracy: 1.0000 - val_loss: 3.9255 - val_accuracy: 0.3762

# <tensorflow.python.keras.callbacks.History at 0x20e3bda0ac8>

test_imgs, test_labels = next(test_batches)
plotImages(test_imgs)
print(test_labels)

print(test_batches.classes)
# array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4,
#        4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6])
test_labels = test_batches.classes

predictions = model.predict(x=test_batches, steps=len(test_batches), verbose=0)
np.round(predictions) # optional?

print(test_labels.shape)
print(predictions.argmax(axis=1).shape)

# confusion matrix
# cm = coonfusion_matrix(y_true=test_labels, y_pred=predictions.argmax(axis=1))
cm = confusion_matrix(y_true=test_batches.classes, y_pred=np.argmax(predictions, axis=-1))

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
            horizontalalignment="center",
            color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

print(test_batches.class_indices)
# {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6}
# 0 angry
# 1 disgusted
# 2 fearful
# 3 happy
# 4 neutral
# 5 sad
# 6 surprised

cm_plot_labels = ['0','1','2','3','4','5','6']
plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')
# Confusion matrix, without normalization
# [[1 3 0 1 0 0 0]
#  [1 3 0 0 0 0 1]
#  [0 0 0 0 0 3 2]
#  [1 1 0 3 0 0 0]
#  [0 1 0 1 1 1 1]
#  [2 1 0 0 0 1 1]
#  [0 0 0 0 0 0 5]]

